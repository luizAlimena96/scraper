# AUM Scraper

Aplica√ß√£o para coleta autom√°tica de AUM (Assets Under Management) de empresas financeiras usando web scraping e IA.

## üöÄ Funcionalidades

- **Leitura de CSV**: Upload de arquivo CSV com empresas
- **Web Scraping**: Fetch de HTML p√∫blico e conte√∫do din√¢mico usando Playwright
- **Extra√ß√£o via IA**: Uso do GPT-4o para extrair AUM com limite de 1500 tokens
- **Controle de Or√ßamento**: Monitoramento de custos da API OpenAI
- **Persist√™ncia**: Banco PostgreSQL com tabelas completas
- **Exporta√ß√£o**: Gera√ß√£o de arquivo Excel com resultados
- **API REST**: Endpoints para monitoramento e controle
- **Interface Web**: Dashboard para upload e monitoramento

## üõ†Ô∏è Stack T√©cnica

- **Backend**: Python 3.11, FastAPI, SQLAlchemy 2.0
- **Banco**: PostgreSQL 15
- **Scraping**: Playwright, BeautifulSoup4
- **IA**: OpenAI GPT-4o
- **Testes**: Pytest
- **Containeriza√ß√£o**: Docker & Docker Compose

## üìã Pr√©-requisitos

- Docker e Docker Compose
- Python 3.11+
- OpenAI API Key

## üöÄ Instala√ß√£o

1. **Clone o reposit√≥rio**:
```bash
git clone https://github.com/luizAlimena96/scraper.git
cd scraper
```

2. **Configure as vari√°veis de ambiente**:
```bash
cp .env.example .env
# Edite o arquivo .env com sua OpenAI API Key
```

3. **Execute com Docker**:
```bash
docker-compose up -d
```

4. **Acesse a aplica√ß√£o**:
- Dashboard: http://localhost:8000
- API Docs: http://localhost:8000/docs

## üìñ Uso

### 1. Upload de CSV
Fa√ßa upload do arquivo CSV com as empresas:
```bash
curl -X POST "http://localhost:8000/upload-csv" \
  -F "file=@companies_formatted.csv"
```

### 2. Iniciar Scraping
```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"company_ids": null}'
```

### 3. Monitorar Progresso
```bash
curl "http://localhost:8000/scrape/status"
```

### 4. Exportar Resultados
```bash
curl "http://localhost:8000/export/excel" -o aum_results.xlsx
```

## üìÅ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # SQLAlchemy models
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py           # Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ services.py          # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py           # Web scraping
‚îÇ   ‚îú‚îÄ‚îÄ ai_extractor.py      # AI extraction
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings
‚îÇ   ‚îú‚îÄ‚îÄ database.py          # DB connection
‚îÇ   ‚îî‚îÄ‚îÄ static/              # Web dashboard
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ alembic/                 # Database migrations
‚îú‚îÄ‚îÄ docker-compose.yml       # Docker setup
‚îú‚îÄ‚îÄ Dockerfile              # Docker image
‚îú‚îÄ‚îÄ requirements.txt        # Dependencies
‚îú‚îÄ‚îÄ sample_companies.csv    # Example data
‚îî‚îÄ‚îÄ README.md              # Documentation
```

## üîß Caracter√≠sticas T√©cnicas

### Sele√ß√£o Inteligente de Conte√∫do
- `extract_relevant_chunks()`: Extrai par√°grafos relevantes usando regex e keywords
- Limite de 1200 tokens antes do prompt
- Keywords: "AUM", "Assets under management", "patrim√¥nio sob gest√£o"

### Controle de Cr√©dito GPT
- `check_budget_and_run()`: Verifica or√ßamento di√°rio
- Alerta quando > 80% do budget
- Bloqueia execu√ß√µes quando budget excedido

### Reconcilia√ß√£o de Unidades
- Converte valores (mi, bi) para formato padronizado (ex.: 2.3e9)
- Suporte a R$, US$, bilh√µes, milh√µes

## üß™ Testes

Execute os testes com cobertura:
```bash
docker exec scraper-backend-1 python -m pytest tests/ -v
```

## üìä API Endpoints

### Empresas
- `POST /upload-csv` - Upload de CSV
- `GET /companies` - Listar empresas

### Scraping
- `POST /scrape` - Iniciar scraping
- `GET /scrape/status` - Status do scraping
- `POST /rescrape/{company_id}` - Re-scrape de empresa espec√≠fica

### Resultados
- `GET /aum-snapshots` - Snapshots de AUM
- `GET /scrape-logs` - Logs de scraping
- `GET /export/excel` - Exportar para Excel

### Admin
- `GET /usage/today` - Consumo de tokens hoje

## üìà Monitoramento

- **RabbitMQ UI**: http://localhost:15672
- **API Docs**: http://localhost:8000/docs
- **Dashboard**: http://localhost:8000

## ‚ö†Ô∏è Observa√ß√µes Importantes

- Monitoramento de custos da API OpenAI
- Uso de prompt engineering e chunking inteligentes
- Registro de "NAO_DISPONIVEL" quando AUM n√£o encontrado
- Rate limiting para evitar bloqueios
- Controle de paralelismo
