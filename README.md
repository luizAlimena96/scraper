# AUM Scraper

AplicaÃ§Ã£o para coleta automÃ¡tica de AUM (Assets Under Management) de empresas financeiras usando web scraping e IA.

## ğŸš€ Funcionalidades

- **Leitura de CSV**: Upload de arquivo CSV com empresas
- **Web Scraping**: Fetch de HTML pÃºblico e conteÃºdo dinÃ¢mico usando Playwright
- **ExtraÃ§Ã£o via IA**: Uso do GPT-4o para extrair AUM com limite de 1500 tokens
- **Controle de OrÃ§amento**: Monitoramento de custos da API OpenAI
- **PersistÃªncia**: Banco PostgreSQL com tabelas completas
- **ExportaÃ§Ã£o**: GeraÃ§Ã£o de arquivo Excel com resultados
- **API REST**: Endpoints para monitoramento e controle
- **Interface Web**: Dashboard para upload e monitoramento

## ğŸ› ï¸ Stack TÃ©cnica

- **Backend**: Python 3.11, FastAPI, SQLAlchemy 2.0
- **Banco**: PostgreSQL 15
- **Scraping**: Playwright, BeautifulSoup4
- **IA**: OpenAI GPT-4o
- **Testes**: Pytest
- **ContainerizaÃ§Ã£o**: Docker & Docker Compose

## ğŸ“‹ PrÃ©-requisitos

- Docker e Docker Compose
- Python 3.11+
- OpenAI API Key

## ğŸš€ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**:
```bash
git clone https://github.com/luizAlimena96/scraper.git
cd scraper
```

2. **Configure as variÃ¡veis de ambiente**:
```bash
cp .env.example .env
# Edite o arquivo .env com sua OpenAI API Key
```

3. **Execute com Docker**:
```bash
docker-compose up -d
```

4. **Acesse a aplicaÃ§Ã£o**:
- Dashboard: http://localhost:8000
- API Docs: http://localhost:8000/docs

## ğŸ“– Uso

### 1. Upload de CSV
FaÃ§a upload do arquivo CSV com as empresas:
```bash
curl -X POST "http://localhost:8000/upload-csv" \
  -F "file=@companies_formatted.csv"
```

### 2. Iniciar Scraping
```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"company_ids": null}'
```

### 3. Monitorar Progresso
```bash
curl "http://localhost:8000/scrape/status"
```

### 4. Exportar Resultados
```bash
curl "http://localhost:8000/export/excel" -o aum_results.xlsx
```

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ models.py            # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py           # Pydantic schemas
â”‚   â”œâ”€â”€ services.py          # Business logic
â”‚   â”œâ”€â”€ scraper.py           # Web scraping
â”‚   â”œâ”€â”€ ai_extractor.py      # AI extraction
â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”œâ”€â”€ database.py          # DB connection
â”‚   â””â”€â”€ static/              # Web dashboard
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ alembic/                 # Database migrations
â”œâ”€â”€ docker-compose.yml       # Docker setup
â”œâ”€â”€ Dockerfile              # Docker image
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ sample_companies.csv    # Example data
â””â”€â”€ README.md              # Documentation
```

## ğŸ”§ CaracterÃ­sticas TÃ©cnicas

### SeleÃ§Ã£o Inteligente de ConteÃºdo
- `extract_relevant_chunks()`: Extrai parÃ¡grafos relevantes usando regex e keywords
- Limite de 1200 tokens antes do prompt
- Keywords: "AUM", "Assets under management", "patrimÃ´nio sob gestÃ£o"

### Controle de CrÃ©dito GPT
- `check_budget_and_run()`: Verifica orÃ§amento diÃ¡rio
- Alerta quando > 80% do budget
- Bloqueia execuÃ§Ãµes quando budget excedido

### ReconciliaÃ§Ã£o de Unidades
- Converte valores (mi, bi) para formato padronizado (ex.: 2.3e9)
- Suporte a R$, US$, bilhÃµes, milhÃµes

## ğŸ§ª Testes

Execute os testes com cobertura:
```bash
docker exec scraper-backend-1 python -m pytest tests/ -v
```

## ğŸ“Š API Endpoints

### Empresas
- `POST /upload-csv` - Upload de CSV
- `GET /companies` - Listar empresas

### Scraping
- `POST /scrape` - Iniciar scraping
- `GET /scrape/status` - Status do scraping
- `POST /rescrape/{company_id}` - Re-scrape de empresa especÃ­fica

### Resultados
- `GET /aum-snapshots` - Snapshots de AUM
- `GET /scrape-logs` - Logs de scraping
- `GET /export/excel` - Exportar para Excel

### Admin
- `GET /usage/today` - Consumo de tokens hoje

## ğŸ“ˆ Monitoramento

- **RabbitMQ UI**: http://localhost:15672
- **API Docs**: http://localhost:8000/docs
- **Dashboard**: http://localhost:8000

## âš ï¸ ObservaÃ§Ãµes Importantes

- Monitoramento de custos da API OpenAI
- Uso de prompt engineering e chunking inteligentes
- Registro de "NAO_DISPONIVEL" quando AUM nÃ£o encontrado
- Rate limiting para evitar bloqueios
- Controle de paralelismo

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ“ Contato

- Projeto: [https://github.com/seu-usuario/aum-scraper](https://github.com/seu-usuario/aum-scraper)

---

Desenvolvido para o processo seletivo Dev Full-Stack.
